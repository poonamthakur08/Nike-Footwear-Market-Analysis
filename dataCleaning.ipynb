{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\poona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\poona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud\n",
    "from pathlib import Path\n",
    "from pandas import DataFrame\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\CORPUS')\n",
    "project_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project')\n",
    "data_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\Data')\n",
    "labels = [\"+Nike +footwear +market +ranking\",\n",
    "\"+global +footwear +market +leader\",\n",
    "\"+top +brand in +footwear +industry\",\n",
    "\"+Nike +market +share in +footwear\",\n",
    "\"Nike footwear\",\n",
    "\"footwear industry\"]\n",
    "endpoint=\"https://newsapi.org/v2/everything\"\n",
    "filename=\"newsApiData.csv\"\n",
    "nytimes_data =  \"fav_sneaker_data.txt\"\n",
    "nike_vs_adidas_data =  \"nike_vs_adidas_data.txt\"\n",
    "nike_reviews_data = \"nike_reviews_data.csv\"\n",
    "twitter_data = \"twitter_data.csv\"\n",
    "filtered_twitter_data = \"filtered_twitter_data.csv\"\n",
    "ct_vectorized_df_cols =[]\n",
    "tidf_vectorized_df_cols =[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countVectorizer(df,col, add_labels = False):\n",
    "    MyVect_CV=CountVectorizer(input=\"content\",stop_words=\"english\",max_features=50)\n",
    "    Vect_CV = MyVect_CV.fit_transform(df[col])\n",
    "    ColumnNamesCV=MyVect_CV.get_feature_names_out()\n",
    "    ct_vectorized_df_cols = ColumnNamesCV\n",
    "    ct_vectorized_df=pd.DataFrame(Vect_CV.toarray(),columns=ColumnNamesCV)\n",
    "\n",
    "    if add_labels:\n",
    "        ct_vectorized_df = concat_labels_to_df(df['Label'],ct_vectorized_df)\n",
    "    ct_vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r'[^A-Za-z]+', \" \", str_input).lower().split()\n",
    "    return words\n",
    "\n",
    "def tfidfvectorizer(df,col, add_labels = False):\n",
    "    ## Tfidf Vectorizer\n",
    "    MyVect_TF=TfidfVectorizer(input='content', stop_words='english', tokenizer = stemming_tokenizer,max_features=1000)\n",
    "    Vect_TF = MyVect_TF.fit_transform(df[col])\n",
    "    ColumnNamesTF=MyVect_TF.get_feature_names_out()\n",
    "    tidf_vectorized_df_cols = ColumnNamesTF\n",
    "    tidf_vectorized_df=pd.DataFrame(Vect_TF.toarray(),columns=ColumnNamesTF)\n",
    "\n",
    "    if add_labels:\n",
    "        tidf_vectorized_df = concat_labels_to_df(df['Label'],tidf_vectorized_df)   \n",
    "    tidf_vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    word_net_lemmatizer = WordNetLemmatizer()\n",
    "    words = str(text).split()\n",
    "    words = [word_net_lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def stemming_words(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = str(text).split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_labels_to_df(labelList,vectorized_df):\n",
    "    Labels_DF = DataFrame(labelList,columns=['LABEL'])\n",
    "    dfs = [Labels_DF, vectorized_df]\n",
    "    return pd.concat(dfs,axis=1, join='inner')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://r-graph-gallery.com/196-the-wordcloud2-library.html\n",
    "def visualizeCountVect():\n",
    "    text = \" \".join(title for title in ct_vectorized_df_cols)\n",
    "    word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def visualizeTfidfVect():\n",
    "    text = \" \".join(title for title in tidf_vectorized_df_cols)\n",
    "    word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poona\\AppData\\Local\\Temp\\ipykernel_30500\\2826190069.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  news_df = pd.read_csv(news_path, error_bad_lines=False)\n",
      "c:\\Users\\poona\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "news_path = data_path.joinpath(filename)\n",
    "news_df = pd.read_csv(news_path, error_bad_lines=False)\n",
    "news_df['Description'] = news_df['Description'].apply(lemmatization)\n",
    "news_df['Description'] = news_df['Description'].apply(stemming_words)\n",
    "countVectorizer(news_df,'Description',True)\n",
    "tfidfvectorizer(news_df,'Description',True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poona\\AppData\\Local\\Temp\\ipykernel_14652\\2883589694.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  reviews_df = pd.read_csv(review_path, error_bad_lines=False,encoding=\"utf-8\")\n",
      "Skipping line 14: expected 6 fields, saw 7\n",
      "Skipping line 23: expected 6 fields, saw 7\n",
      "Skipping line 30: expected 6 fields, saw 8\n",
      "Skipping line 46: expected 6 fields, saw 7\n",
      "Skipping line 50: expected 6 fields, saw 8\n",
      "Skipping line 55: expected 6 fields, saw 9\n",
      "Skipping line 71: expected 6 fields, saw 8\n",
      "Skipping line 73: expected 6 fields, saw 8\n",
      "Skipping line 79: expected 6 fields, saw 7\n",
      "Skipping line 86: expected 6 fields, saw 13\n",
      "Skipping line 98: expected 6 fields, saw 14\n",
      "Skipping line 100: expected 6 fields, saw 9\n",
      "Skipping line 108: expected 6 fields, saw 9\n",
      "Skipping line 115: expected 6 fields, saw 8\n",
      "Skipping line 118: expected 6 fields, saw 16\n",
      "Skipping line 119: expected 6 fields, saw 12\n",
      "Skipping line 121: expected 6 fields, saw 9\n",
      "Skipping line 132: expected 6 fields, saw 7\n",
      "Skipping line 136: expected 6 fields, saw 7\n",
      "Skipping line 140: expected 6 fields, saw 8\n",
      "Skipping line 144: expected 6 fields, saw 12\n",
      "Skipping line 149: expected 6 fields, saw 9\n",
      "Skipping line 154: expected 6 fields, saw 10\n",
      "Skipping line 157: expected 6 fields, saw 11\n",
      "Skipping line 158: expected 6 fields, saw 9\n",
      "Skipping line 161: expected 6 fields, saw 12\n",
      "Skipping line 166: expected 6 fields, saw 15\n",
      "Skipping line 167: expected 6 fields, saw 9\n",
      "Skipping line 171: expected 6 fields, saw 9\n",
      "Skipping line 175: expected 6 fields, saw 11\n",
      "Skipping line 177: expected 6 fields, saw 7\n",
      "Skipping line 180: expected 6 fields, saw 11\n",
      "Skipping line 185: expected 6 fields, saw 12\n",
      "Skipping line 189: expected 6 fields, saw 9\n",
      "Skipping line 194: expected 6 fields, saw 14\n",
      "Skipping line 203: expected 6 fields, saw 11\n",
      "Skipping line 209: expected 6 fields, saw 15\n",
      "Skipping line 216: expected 6 fields, saw 7\n",
      "Skipping line 225: expected 6 fields, saw 7\n",
      "Skipping line 236: expected 6 fields, saw 10\n",
      "Skipping line 237: expected 6 fields, saw 8\n",
      "Skipping line 244: expected 6 fields, saw 14\n",
      "Skipping line 249: expected 6 fields, saw 8\n",
      "Skipping line 251: expected 6 fields, saw 7\n",
      "Skipping line 253: expected 6 fields, saw 9\n",
      "Skipping line 259: expected 6 fields, saw 8\n",
      "Skipping line 260: expected 6 fields, saw 8\n",
      "Skipping line 274: expected 6 fields, saw 7\n",
      "Skipping line 279: expected 6 fields, saw 13\n",
      "Skipping line 281: expected 6 fields, saw 12\n",
      "Skipping line 283: expected 6 fields, saw 8\n",
      "Skipping line 285: expected 6 fields, saw 9\n",
      "Skipping line 289: expected 6 fields, saw 7\n",
      "Skipping line 290: expected 6 fields, saw 11\n",
      "Skipping line 294: expected 6 fields, saw 19\n",
      "Skipping line 296: expected 6 fields, saw 7\n",
      "Skipping line 298: expected 6 fields, saw 7\n",
      "Skipping line 311: expected 6 fields, saw 11\n",
      "Skipping line 313: expected 6 fields, saw 8\n",
      "Skipping line 314: expected 6 fields, saw 7\n",
      "Skipping line 316: expected 6 fields, saw 11\n",
      "Skipping line 317: expected 6 fields, saw 7\n",
      "Skipping line 320: expected 6 fields, saw 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_path = data_path.joinpath(nike_reviews_data)\n",
    "reviews_df = pd.read_csv(review_path, error_bad_lines=False,encoding=\"utf-8\")\n",
    "\n",
    "reviews_df['Review'] = reviews_df['Review'].apply(lemmatization)\n",
    "reviews_df['Review'] = reviews_df['Review'].apply(stemming_words)\n",
    "countVectorizer(reviews_df,'Review')\n",
    "tfidfvectorizer(reviews_df,'Review')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poona\\AppData\\Local\\Temp\\ipykernel_30500\\111497263.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  twitter_df = pd.read_csv(twitter_path, error_bad_lines=False)\n",
      "c:\\Users\\poona\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "twitter_path = data_path.joinpath(twitter_data)\n",
    "twitter_df = pd.read_csv(twitter_path, error_bad_lines=False)\n",
    "twitter_df['Tweet Text'] = twitter_df['Tweet Text'].apply(lemmatization)\n",
    "twitter_df['Tweet Text'] = twitter_df['Tweet Text'].apply(stemming_words)\n",
    "countVectorizer(twitter_df,'Tweet Text')\n",
    "tfidfvectorizer(twitter_df,'Tweet Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afe57345d6b78de11303afd6d7a92989906a5fc5cba5d6e46dc784aa2e3ade50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
