{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTnp7T88BfhG",
        "outputId": "68528cb0-dcec-4222-99b2-bfba49eb7d17"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "# viz libs\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning) \n",
        "warnings.filterwarnings('ignore', category=FutureWarning) \n",
        "warnings.filterwarnings('ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpSJ9kVWBfhK"
      },
      "outputs": [],
      "source": [
        "corpus_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\CORPUS')\n",
        "data_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\Data')\n",
        "filename=\"newsApiData.csv\"\n",
        "nytimes_data =  \"fav_sneaker_data.txt\"\n",
        "nike_vs_adidas_data =  \"nike_vs_adidas_data.txt\"\n",
        "nike_reviews_data = \"nike_reviews_data.csv\"\n",
        "twitter_data = \"twitter_data.csv\"\n",
        "filtered_twitter_data = \"filtered_twitter_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrMRPMKQBfhL"
      },
      "outputs": [],
      "source": [
        "def lemmatization(text):\n",
        "    word_net_lemmatizer = WordNetLemmatizer()\n",
        "    words = str(text).split()\n",
        "    words = [word_net_lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def stemming_words(text):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    words = str(text).split()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTNSjoK7BfhL"
      },
      "outputs": [],
      "source": [
        "review_path = data_path.joinpath(nike_reviews_data)\n",
        "reviews_df = pd.read_csv(review_path, error_bad_lines=False,encoding=\"utf-8\")\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(lemmatization)\n",
        "reviews_df['Review'] = reviews_df['Review'].apply(stemming_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqWhOloSBfhM"
      },
      "outputs": [],
      "source": [
        "MyCountV=CountVectorizer(\n",
        "        input=\"content\",  ## because we have a csv file\n",
        "        lowercase = True,\n",
        "        token_pattern = r'\\b[a-zA-Z]{3,}\\b', \n",
        "        stop_words = \"english\",\n",
        "        max_features=100\n",
        "        )\n",
        "\n",
        "## Use your CV \n",
        "MyDTM = MyCountV.fit_transform(reviews_df['Review'])  # create a sparse matrix\n",
        "ColumnNames=MyCountV.get_feature_names_out()\n",
        "\n",
        "## Build the data frame\n",
        "MyDTM_DF=pd.DataFrame(MyDTM.toarray(),columns=ColumnNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Z7IRrlwoBfhQ",
        "outputId": "3a8410fb-63ef-4e2c-c574-4c53b6c636a8"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df = 0.05, max_df=0.95,\n",
        "                             lowercase = True,\n",
        "        token_pattern = r'\\b[a-zA-Z]{4,}\\b', \n",
        "        stop_words = \"english\" )\n",
        "# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
        "X = vectorizer.fit_transform(reviews_df['Review'])\n",
        "ColumnNames=vectorizer.get_feature_names_out()\n",
        "\n",
        "## Build the data frame\n",
        "Mytfid_DF=pd.DataFrame(X.toarray(),columns=ColumnNames)\n",
        "Mytfid_DF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Mytfid_DF.to_csv('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\Data\\clustering_sample_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA3kfOJdBfhQ"
      },
      "outputs": [],
      "source": [
        "# initialize kmeans with 3 centroids\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "# fit the model\n",
        "kmeans.fit(X)\n",
        "# store cluster labels in a variable\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# initialize kmeans with 4 centroids\n",
        "kmeans4 = KMeans(n_clusters=4, random_state=42)\n",
        "# fit the model\n",
        "kmeans4.fit(X)\n",
        "# store cluster labels in a variable\n",
        "clusters4 = kmeans4.labels_\n",
        "\n",
        "# initialize kmeans with 2 centroids\n",
        "kmeans2 = KMeans(n_clusters=2, random_state=42)\n",
        "# fit the model\n",
        "kmeans2.fit(X)\n",
        "# store cluster labels in a variable\n",
        "clusters2 = kmeans2.labels_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRGHqbjoBfhR"
      },
      "outputs": [],
      "source": [
        "# initialize PCA with 2 components\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "# pass our X to the pca and store the reduced vectors into pca_vecs\n",
        "pca_vecs = pca.fit_transform(X.toarray())\n",
        "# save our two dimensions into x0 and x1\n",
        "x0 = pca_vecs[:, 0]\n",
        "x1 = pca_vecs[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58HQRhUmBfhR"
      },
      "outputs": [],
      "source": [
        "reviews_df['cluster3'] = clusters\n",
        "reviews_df['cluster4'] = clusters4\n",
        "reviews_df['cluster2'] = clusters2\n",
        "reviews_df['x0'] = x0\n",
        "reviews_df['x1'] = x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "X2QGfUgGBfhR",
        "outputId": "3c55e8e4-69b5-48d6-b206-50ab028f019f"
      },
      "outputs": [],
      "source": [
        "def get_top_keywords(n_terms,clusters):\n",
        "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
        "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
        "    terms = vectorizer.get_feature_names_out() # access tf-idf terms\n",
        "    for i,r in df.iterrows():\n",
        "        print('\\nCluster {}'.format(i))\n",
        "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
        "            \n",
        "get_top_keywords(10,clusters)\n",
        "\n",
        "# map clusters to appropriate labels \n",
        "cluster_map = {0: \"features\", 1: \"opinion\", 2: \"service\"}\n",
        "# apply mapping\n",
        "reviews_df['cluster'] = reviews_df['cluster'].map(cluster_map)\n",
        "\n",
        "# set image size\n",
        "plt.figure(figsize=(12, 7))\n",
        "# set a title\n",
        "plt.title(\"clustering k=3\", fontdict={\"fontsize\": 18})\n",
        "# set axes names\n",
        "plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
        "plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
        "# create scatter plot with seaborn, where hue is the class used to group the data\n",
        "sns.scatterplot(data=reviews_df, x='x0', y='x1', hue='cluster', palette=\"viridis\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEiNqYtqBfhS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "iAEZHK7nBfhS",
        "outputId": "bb566f48-a4ad-46ea-b7b4-b139de88ab73"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "sSw6u2sWBfhT",
        "outputId": "cc211017-84eb-45aa-ddd6-882a9d3a2333"
      },
      "outputs": [],
      "source": [
        "\"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
        "df4 = pd.DataFrame(X.todense()).groupby(clusters4).mean() # groups the TF-IDF vector by cluster\n",
        "terms4 = vectorizer.get_feature_names_out() # access tf-idf terms\n",
        "for i,r in df4.iterrows():\n",
        "    print('\\nCluster {}'.format(i))\n",
        "    print(','.join([terms4[t] for t in np.argsort(r)[-10:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
        "\n",
        "# map clusters to appropriate labels \n",
        "cluster_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\"}\n",
        "# apply mapping\n",
        "reviews_df['cluster4'] = reviews_df['cluster4'].map(cluster_map)\n",
        "\n",
        "# set image size\n",
        "plt.figure(figsize=(12, 7))\n",
        "# set a title\n",
        "plt.title(\"clustering k=4\", fontdict={\"fontsize\": 18})\n",
        "# set axes names\n",
        "plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
        "plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
        "# create scatter plot with seaborn, where hue is the class used to group the data\n",
        "sns.scatterplot(data=reviews_df, x='x0', y='x1', hue='cluster4', palette=\"viridis\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "RD9yrJ4GF9N6",
        "outputId": "48abe401-263b-4b3f-ef9a-0ce85a0bc473"
      },
      "outputs": [],
      "source": [
        "\"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
        "df2 = pd.DataFrame(X.todense()).groupby(clusters2).mean() # groups the TF-IDF vector by cluster\n",
        "terms4 = vectorizer.get_feature_names_out() # access tf-idf terms\n",
        "for i,r in df2.iterrows():\n",
        "    print('\\nCluster {}'.format(i))\n",
        "    print(','.join([terms4[t] for t in np.argsort(r)[-10:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
        "\n",
        "# map clusters to appropriate labels \n",
        "cluster_map = {0: \"1\", 1: \"2\"}\n",
        "# apply mapping\n",
        "reviews_df['cluster2'] = reviews_df['cluster2'].map(cluster_map)\n",
        "\n",
        "# set image size\n",
        "plt.figure(figsize=(12, 7))\n",
        "# set a title\n",
        "plt.title(\"clustering k=2\", fontdict={\"fontsize\": 18})\n",
        "# set axes names\n",
        "plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
        "plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
        "# create scatter plot with seaborn, where hue is the class used to group the data\n",
        "sns.scatterplot(data=reviews_df, x='x0', y='x1', hue='cluster2', palette=\"viridis\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3XCcb-wBfhT",
        "outputId": "57b262fe-f673-411f-e706-adea7aba851f"
      },
      "outputs": [],
      "source": [
        "My_Orig_DF = MyDTM_DF\n",
        "#My_KMean= KMeans(n_clusters=3)\n",
        "#My_KMean.fit(My_Orig_DF)\n",
        "My_labels=kmeans.predict(X)\n",
        "\n",
        "# initialize kmeans with 3 centroids\n",
        "#kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "# fit the model\n",
        "#kmeans.fit(X)\n",
        "# store cluster labels in a variable\n",
        "#clusters = kmeans.labels_\n",
        "print(\"Silhouette Score for k = 3 \\n\",silhouette_score(My_Orig_DF, My_labels))\n",
        "\n",
        "#cosdist = 1 - cosine_similarity(MyDTM)\n",
        "#print(cosdist)\n",
        "#print(np.round(cosdist,3))  #cos dist should be .02\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPQnT7T1BfhT"
      },
      "outputs": [],
      "source": [
        "#sns.scatterplot(My_Orig_DF.iloc[0],My_Orig_DF.iloc[1], hue=My_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "1SDTBnByBfhU",
        "outputId": "fbde73f3-fb66-476e-b870-1b0904cb2765"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        " \n",
        "fig, ax = plt.subplots(2,2, figsize=(15,8))\n",
        "for i in [2, 3, 4, 5]:\n",
        "    '''\n",
        "    Create KMeans instance for different number of clusters\n",
        "    '''\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
        "    q, mod = divmod(i, 2)\n",
        "    '''\n",
        "    Create SilhouetteVisualizer instance with KMeans instance\n",
        "    Fit the visualizer\n",
        "    '''\n",
        "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
        "    visualizer.fit(My_Orig_DF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM5BgmlFBfhU"
      },
      "outputs": [],
      "source": [
        "#pip install yellowbrick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhGkurSVBfhX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "afe57345d6b78de11303afd6d7a92989906a5fc5cba5d6e46dc784aa2e3ade50"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
