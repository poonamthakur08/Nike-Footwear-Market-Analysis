---
title: "associationRuleMining"
author: "PoonamParagThakur"
date: '2023-02-28'
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rlang)
library(RCurl)
library(tokenizers)
library(dplyr)
library(arules)
library(arulesViz)
library(tokenizers)
library(tidyverse)
library(plyr)
library(dplyr)
library(ggplot2)
library(stopwords)
library(networkD3)
library(tm)
library(textstem)

#install_github("mhahsler/arulesViz")
#detach("package:arules", unload=TRUE)
#library("arules")
```


```{r}
review_df <- read.csv('E:/Masters_Coursework/Sem4/Text Mining/Project/Data/nike_adidas_reviews_data.csv',header=TRUE,sep=",")
nike_adidas_Corpus <- Corpus(DirSource('E:/Masters_Coursework/Sem4/Text Mining/Project/CORPUS'))

```

```{r}
trans <- file("E:/Masters_Coursework/Sem4/Text Mining/Project/Data/ReviewTransaction.csv",open = "a")
for(i in 1:nrow(review_df)){
  Tokens <- tokenize_words(review_df$Review[i], stopwords = stopwords::stopwords("en"), lowercase = TRUE, strip_punct = TRUE, simplify = TRUE)
  cat(unlist(str_squish(Tokens)), "\n", file = trans, sep = ",")
}
close(trans)
```

```{r}

reviewTransac <- read.transactions("E:/Masters_Coursework/Sem4/Text Mining/Project/Data/ReviewTransaction.csv", sep =",",format("basket"), rm.duplicates = TRUE)
#inspect(reviewTransac)
sample_trans <- sample(reviewTransac,100)
summary(sample_trans)
```


```{r}
reviewTransacDF <- read.csv("ReviewTransaction.csv", sep =",", header=FALSE)

reviewTransacDF<-reviewTransacDF %>% mutate_all(as.character)
(str(reviewTransacDF))

reviewTransacDF[reviewTransacDF == "t"] <- ""
reviewTransacDF[reviewTransacDF == "ll"] <- ""
reviewTransacDF[reviewTransacDF == "s"] <- ""
reviewTransacDF[reviewTransacDF == "u"] <- ""
reviewTransacDF[reviewTransacDF == "re"] <- ""
reviewTransacDF[reviewTransacDF == "jd"] <- ""
reviewTransacDF[reviewTransacDF == "l"] <- ""
```

```{r}
MyDF<-NULL
MyDF1<-NULL
for (i in 1:ncol(reviewTransacDF)){
  numberList=c() 
  lenList=c()
  
  numberList=c(numberList,grepl("[[:digit:]]", reviewTransacDF[[i]]))
  MyDF<-cbind(MyDF,numberList)
  
  lenList=c(lenList,(nchar(reviewTransacDF[[i]])<4 | nchar(reviewTransacDF[[i]])>11))
  MyDF1<-cbind(MyDF1,lenList) 
}
reviewTransacDF[MyDF] <- ""
reviewTransacDF[MyDF1] <- ""
(head(reviewTransacDF,10))
# save the dataframe using the write table command 
write.table(reviewTransacDF, file = "clean_review_transac.csv", col.names = FALSE, 
            row.names = FALSE, sep = ",")
```

```{r}
SmallCorpus = nike_adidas_Corpus
#(ndocs<-length(SmallCorpus))

## Do some clean-up.............
SmallCorpus <- tm_map(SmallCorpus, content_transformer(tolower))
SmallCorpus <- tm_map(SmallCorpus, removePunctuation)
## Remove all Stop Words
SmallCorpus <- tm_map(SmallCorpus, removeWords, stopwords("english"))

## You can also remove words that you do not want
#MyStopWords <- c("and","like", "very", "can", "I", "also", "lot")
#SmallCorpus <- tm_map(SmallCorpus, removeWords, MyStopWords)
SmallCorpus <- tm_map(SmallCorpus, lemmatize_strings)
##-------------------------------------------------------------

## Convert to Document Term Matrix  and TERM document matrix
## Each has its own purpose.

## DOCUMENT Term Matrix  (Docs are rows)
SmallCorpus_DTM <- DocumentTermMatrix(SmallCorpus,
                                 control = list(
                                   stopwords = TRUE, ## remove normal stopwords
                                   wordLengths=c(3, 10), ## get rid of words of len 2 or smaller or larger than 15
                                   removePunctuation = TRUE,
                                   removeNumbers = TRUE,
                                   tolower=TRUE
                                   #stemming = TRUE,
                                 ))

#inspect(SmallCorpus_DTM)

## TERM Document Matrix  (words are rows)
SmallCorpus_TERM_DM <- TermDocumentMatrix(SmallCorpus,
                                      control = list(
                                        stopwords = TRUE, ## remove normal stopwords
                                        wordLengths=c(3, 10), ## get rid of words of len 2 or smaller or larger than 15
                                        removePunctuation = TRUE,
                                        removeNumbers = TRUE,
                                        tolower=TRUE
                                        #stemming = TRUE,
                                      ))

#inspect(SmallCorpus_TERM_DM)
```

```{r}

reviewTransac <- read.transactions("E:/Masters_Coursework/Sem4/Text Mining/Project/Data/clean_review_transac.csv", sep =",", 
                                format("basket"),  rm.duplicates = TRUE)
```

```{r}
trans_rules = arules::apriori(reviewTransac, 
                                   parameter = list(support=.06, conf=.1, minlen=2))
View(trans_rules)

```

```{r}
SortedRules_conf <- sort(trans_rules, by="confidence", decreasing=TRUE)
#inspectDT(SortedRules_conf[1:15])
```

```{r}
SortedRules_sup <- sort(trans_rules, by="support", decreasing=TRUE)
#inspectDT(SortedRules_sup[1:15],)
```

```{r}
SortedRules_lift <- sort(trans_rules, by="lift", decreasing=TRUE)
#inspectDT(SortedRules_lift[1:15])
```

```{r}
plot(SortedRules_sup[1:30],method="graph",engine='interactive', shading="support") 
plot(SortedRules_conf[1:30],method="graph",engine='interactive',shading="confidence")
plot(SortedRules_lift[1:30],method="graph",engine='interactive',shading="lift")
```

```{r}
#convert rules to dataframe
Rules_DF2<-DATAFRAME(trans_rules, separate = TRUE)
(head(Rules_DF2))
str(Rules_DF2)

#convert to char
Rules_DF2$LHS<-as.character(Rules_DF2$LHS)
Rules_DF2$RHS<-as.character(Rules_DF2$RHS)

#Remove all {}
Rules_DF2[] <- lapply(Rules_DF2, gsub, pattern='[{]', replacement='')
Rules_DF2[] <- lapply(Rules_DF2, gsub, pattern='[}]', replacement='')

## USING LIFT  as weight
Rules_L<-Rules_DF2[c(1,2,5)]
# rename columns
names(Rules_L) <- c("SourceName", "TargetName", "Weight")
head(Rules_L,30)

## USING SUP
Rules_S<-Rules_DF2[c(1,2,3)]
# rename columns
names(Rules_S) <- c("SourceName", "TargetName", "Weight")
head(Rules_S,30)

## USING CONF
Rules_C<-Rules_DF2[c(1,2,4)]
# rename columns
names(Rules_C) <- c("SourceName", "TargetName", "Weight")
head(Rules_C,30)

## CHoose and set
#Rules_Sup<-Rules_C
#Rules_Sup<-Rules_L
Rules_Sup<-Rules_S
```

```{r}

#############       Build a NetworkD3 edgeList and nodeList    ############

#edgeList<-Rules_Sup
# Create a graph. Use simplyfy to ensure that there are no duplicated edges or self loops
#MyGraph <- igraph::simplify(igraph::graph.data.frame(edgeList, directed=TRUE))
#plot(MyGraph)

############################### BUILD THE NODES & EDGES ####################################
(edgeList<-Rules_Sup)
(MyGraph <- igraph::simplify(igraph::graph.data.frame(edgeList, directed=TRUE)))

nodeList <- data.frame(ID = c(0:(igraph::vcount(MyGraph) - 1)), 
                       # because networkD3 library requires IDs to start at 0
                       nName = igraph::V(MyGraph)$name)
## Node Degree
(nodeList <- cbind(nodeList, nodeDegree=igraph::degree(MyGraph, 
                    v = igraph::V(MyGraph), mode = "all")))

```

```{r}
## Betweenness
BetweenNess <- igraph::betweenness(MyGraph, 
                                   v = igraph::V(MyGraph), 
                                   directed = TRUE) 

(nodeList <- cbind(nodeList, nodeBetweenness=BetweenNess))

## This can change the BetweenNess value if needed
#BetweenNess<-BetweenNess/100

getNodeID <- function(x){
  which(x == igraph::V(MyGraph)$name) - 1  #IDs start at 0
}
## UPDATE THIS !! depending on # choice
(getNodeID("salary")) 

edgeList <- plyr::ddply(
  Rules_Sup, .variables = c("SourceName", "TargetName" , "Weight"), 
  function (x) data.frame(SourceID = getNodeID(x$SourceName), 
                          TargetID = getNodeID(x$TargetName)))

head(edgeList)
nrow(edgeList)

```
```{r}
##############  Dice Sim ################################################
#Calculate Dice similarities between all pairs of nodes
#The Dice similarity coefficient of two vertices is twice 
#the number of common neighbors divided by the sum of the degrees 
#of the vertices. Method dice calculates the pairwise Dice similarities 
#for some (or all) of the vertices. 
DiceSim <- igraph::similarity.dice(MyGraph, vids = igraph::V(MyGraph), mode = "all")
head(DiceSim)

#Create  data frame that contains the Dice similarity between any two vertices
F1 <- function(x) {data.frame(diceSim = DiceSim[x$SourceID +1, x$TargetID + 1])}
#Place a new column in edgeList with the Dice Sim
head(edgeList)
edgeList <- plyr::ddply(edgeList,
                        .variables=c("SourceName", "TargetName", "Weight", 
                                               "SourceID", "TargetID"), 
                        function(x) data.frame(F1(x)))
head(edgeList)
```
```{r}
##################   color #################################################
COLOR_P <- colorRampPalette(c("#00FF00", "#FF0000"), 
                            bias = nrow(edgeList), space = "rgb", 
                            interpolate = "linear")
 COLOR_P
 (colCodes <- COLOR_P(length(unique(edgeList$diceSim))))
 edges_col <- sapply(edgeList$diceSim, 
                     function(x) colCodes[which(sort(unique(edgeList$diceSim)) == x)])
 nrow(edges_col)

```

```{r}
D3_network_Tweets <- networkD3::forceNetwork(
  Links = edgeList, # data frame that contains info about edges
  Nodes = nodeList, # data frame that contains info about nodes
  Source = "SourceID", # ID of source node 
  Target = "TargetID", # ID of target node
  Value = "Weight", # value from the edge list (data frame) that will be used to value/weight relationship amongst nodes
  NodeID = "nName", # value from the node list (data frame) that contains node description we want to use (e.g., node name)
  Nodesize = "nodeBetweenness",  # value from the node list (data frame) that contains value we want to use for a node size
  Group = "nodeDegree",  # value from the node list (data frame) that contains value we want to use for node color
  height = 900, # Size of the plot (vertical)
  width = 1000,  # Size of the plot (horizontal)
  fontSize = 20, # Font size
  linkDistance = networkD3::JS("function(d) { return d.value*1000; }"), # Function to determine distance between any two nodes, uses variables already defined in forceNetwork function (not variables from a data frame)
  linkWidth = networkD3::JS("function(d) { return d.value*5; }"),# Function to determine link/edge thickness, uses variables already defined in forceNetwork function (not variables from a data frame)
  opacity = 9, # opacity
  zoom = TRUE, # ability to zoom when click on the node
  opacityNoHover = 9, # opacity of labels when static
  linkColour = "red"   ###"edges_col"red"# edge colors
) 

# Plot network
#D3_network_Tweets

# Save network as html file
networkD3::saveNetwork(D3_network_Tweets, 
                       "nike_review_sup.html", selfcontained = TRUE)
```