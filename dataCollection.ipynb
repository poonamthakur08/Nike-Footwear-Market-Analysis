{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\poona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\poona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests  ## for getting data from a server GET\n",
    "import re   ## for regular expressions   \n",
    "import numpy as np\n",
    "import pandas as pd ## for dataframes and related\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html  \n",
    "from pathlib import Path\n",
    "import tweepy\n",
    "import nltk\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#To add wait time between requests\n",
    "\n",
    "import configparser\n",
    "import tweepy\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\CORPUS')\n",
    "project_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project')\n",
    "data_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\Data')\n",
    "labels = [\"+Nike +footwear +market +ranking\",\n",
    "\"+global +footwear +market +leader\",\n",
    "\"+top +brand in +footwear +industry\",\n",
    "\"+Nike +market +share in +footwear\",\n",
    "\"Nike footwear\",\n",
    "\"footwear industry\"]\n",
    "endpoint=\"https://newsapi.org/v2/everything\"\n",
    "filename=\"newsApiData.csv\"\n",
    "nytimes_data =  \"fav_sneaker_data.txt\"\n",
    "nike_vs_adidas_data =  \"nike_vs_adidas_data.txt\"\n",
    "nike_reviews_data = \"nike_reviews_data.csv\"\n",
    "nike_adidas_reviews_data = \"nike_adidas_reviews_data.csv\"\n",
    "twitter_data = \"twitter_data.csv\"\n",
    "twitter_labels = [\"footwear market\",\n",
    "\"global footwear leader\",\n",
    "\"top brand in footwear\",\n",
    "\"Nike market\",\n",
    "\"Nike footwear\",\n",
    "\"footwear industry\",\n",
    "\"Leading footwear brand\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEWS API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontxt = {}\n",
    "for topic in labels:\n",
    "    ## Dictionary Structure\n",
    "    URLPost = {'apiKey':'b1c5c9b6caec41e4933b5c6c76c4051b',\n",
    "                'sort_by':'relevancy',\n",
    "               'q':topic,\n",
    "               'pageSize':100\n",
    "    }\n",
    "\n",
    "    response=requests.get(endpoint, URLPost)\n",
    "    print(response)\n",
    "    jsontxt[topic] = response.json()\n",
    "    #print(jsontxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(text):\n",
    "    text=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(' +', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(r'\\\"', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(r'[^a-zA-Z]', \" \", text, flags=re.VERBOSE)\n",
    "    text=text.replace(',', '')\n",
    "    text=' '.join(text.split())\n",
    "    text=re.sub(\"\\n|\\r\", \"\", text)\n",
    "    text= ' '.join([wd for wd in text.split() if len(wd)>3])\n",
    "    return text\n",
    "\n",
    "file_path = data_path.joinpath(filename)\n",
    "MyFILE=open(file_path,\"w\")\n",
    "WriteThis=\"Label,Date,Author,Source,Title,Url,Headline,Content\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "for topic in labels:\n",
    "    articles = jsontxt[topic][\"articles\"]\n",
    "    label = re.sub('[+]',\"\",topic) if \"+\" in topic else topic\n",
    "    for i in range(len(articles)):              \n",
    "        Date=articles[i][\"publishedAt\"]\n",
    "        NewDate=Date.split(\"T\")\n",
    "        Date=NewDate[0]\n",
    "\n",
    "        Author=articles[i][\"author\"]\n",
    "        Author=str(Author)\n",
    "        Author=Author.replace(',', '')\n",
    "\n",
    "        Source=articles[i][\"source\"][\"name\"]\n",
    "\n",
    "        Title=articles[i][\"title\"]\n",
    "        Title=clean_content(str(Title))\n",
    "        \n",
    "        url = articles[i][\"url\"]\n",
    "\n",
    "        Headline=articles[i][\"description\"]\n",
    "        Headline=clean_content(str(Headline))\n",
    "\n",
    "        Content=articles[i][\"content\"]\n",
    "        Content=clean_content(str(Content))\n",
    "        \n",
    "        WriteThis=str(label)+\",\"+str(Date)+\",\"+str(Author)+\",\"+str(Source)+\",\"+ str(Title) + \",\" + str(Url) + \",\" + str(Headline) +\",\" + str(Content) + \"\\n\" \n",
    "        MyFILE.write(WriteThis)\n",
    "            \n",
    "MyFILE.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Websites used for scrapping\n",
    "# https://www.nytimes.com/wirecutter/guides/our-favorite-sneakers/\n",
    "# https://www.trustpilot.com/review/www.nike.com \n",
    "# https://seekingalpha.com/article/4516865-nike-vs-adidas-an-undisputed-leader\n",
    "# https://www.feedough.com/10-biggest-nike-competitors/\n",
    "# https://travel.earth/adidas-vs-nike/ \n",
    "# https://www.unacast.com/post/nike-adidas-puma-foot-traffic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_soupdata(url_link):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36'}\n",
    "    response = requests.get(url_link,headers=headers).text\n",
    "    return BeautifulSoup(response,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.nytimes.com/wirecutter/guides/our-favorite-sneakers/'\n",
    "nytimes_soup = get_website_soupdata(url)\n",
    "info = nytimes_soup.find_all('div',attrs = {'class':'ca523f26 _7cb53a9f _614adc05'})\n",
    "fav_sneakers = []\n",
    "if info is not None and len(info)>0 :\n",
    "    for record in info:\n",
    "        para = record.find('p')\n",
    "        if para is not None:\n",
    "            fav_sneakers.append(para.text) \n",
    "\n",
    "    if len(fav_sneakers) > 0:\n",
    "        file_path = corpus_path.joinpath(nytimes_data)\n",
    "        MyFILE=open(file_path,\"w\")\n",
    "        WriteThis=\"Blog Content\\n\"\n",
    "        MyFILE.write(WriteThis)\n",
    "        MyFILE.writelines(fav_sneakers)\n",
    "        MyFILE.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://travel.earth/adidas-vs-nike/'\n",
    "adidas_nike_soup = get_website_soupdata(url)\n",
    "nike_adidas_info = adidas_nike_soup.find_all('div',attrs = {'class':'td-post-content tagdiv-type'})\n",
    "nike_adidas = []\n",
    "if nike_adidas_info is not None and len(nike_adidas_info)>0 :\n",
    "    for record in nike_adidas_info:\n",
    "        all_para = record.find_all('p')\n",
    "        for val in all_para:\n",
    "            if val is not None:\n",
    "                nike_adidas.append(val.text) \n",
    "\n",
    "    if len(nike_adidas) > 0:\n",
    "        file_path = corpus_path.joinpath(nike_vs_adidas_data)\n",
    "        MyFILE=open(file_path,\"w\",encoding=\"utf-8\")\n",
    "        WriteThis=\"Blog Content\\n\"\n",
    "        MyFILE.write(WriteThis)\n",
    "        MyFILE.writelines(nike_adidas)\n",
    "        MyFILE.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.trustpilot.com/review/www.nike.com',\n",
    "        'https://www.trustpilot.com/review/www.adidas.com']\n",
    "\n",
    "brands  = ['nike', 'adidas']\n",
    "def get_all_reviews_url(brands):\n",
    "    dict  = {}\n",
    "    for brand in brands:\n",
    "        all_url_list = []\n",
    "        url = 'https://www.trustpilot.com/review/www.'+ brand +'.com'\n",
    "        all_url_list.append(url)\n",
    "        for i in range(0,15):\n",
    "            all_url_list.append(url+'?page='+str(i+2))\n",
    "        dict[brand] = all_url_list\n",
    "    return dict\n",
    "\n",
    "all_review_urls_dict = get_all_reviews_url(brands)\n",
    "file_path = data_path.joinpath(nike_adidas_reviews_data)\n",
    "MyFILE=open(file_path,\"w\",encoding=\"utf-8\")\n",
    "WriteThis=\"Brand,Title,Rating,Review\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "for x, y in all_review_urls_dict.items():\n",
    "    for url in y:\n",
    "        nike_soup = get_website_soupdata(url)\n",
    "        nike_reviews = nike_soup.find_all('section',attrs = {'class':'styles_reviewContentwrapper__zH_9M'})\n",
    "        if nike_reviews is not None and len(nike_reviews)>0 :\n",
    "            \n",
    "            for record in nike_reviews:\n",
    "                title_h2 = record.find('h2',attrs = {'class':'typography_heading-s__f7029 typography_appearance-default__AAY17'})\n",
    "                title_h2 = readandCleanData(str(title_h2.text)) if title_h2 is not None else 'N/A'\n",
    "                rating_img = record.find('img')\n",
    "                rating = str(rating_img['alt']) if rating_img is not None else 'N/A'\n",
    "                para_p = record.find('p',attrs = {'class':'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'}) \n",
    "                review = readandCleanData(str(para_p.text)) if para_p is not None else 'N/A'\n",
    "                WriteThis= x + \",\" + title_h2 + \",\" + rating + \",\" + review +\"\\n\"   \n",
    "                MyFILE.write(WriteThis)       \n",
    "            ## CLOSE THE FILE\n",
    "MyFILE.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWITTER API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key = config['twitter']['consumerKey']\n",
    "api_key_secret = config['twitter']['consumerSecret']\n",
    "\n",
    "access_token = config['twitter']['accessToken']\n",
    "access_token_secret = config['twitter']['accessTokenSecret']\n",
    "\n",
    "\n",
    "# authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_jsontxt = {}\n",
    "for topic in twitter_labels:\n",
    "    topic = re.sub('[+]',\"\",topic) if \"+\" in topic else topic\n",
    "    try:\n",
    "        fetched_tweets = api.search_tweets(q = topic,lang=\"en\", count = 100,text: \"full_text\")\n",
    "        tweets_jsontxt[topic] = fetched_tweets\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readandCleanData(string_value):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    string_value.lstrip()\n",
    "    string_value.rstrip()\n",
    "    string_value.strip()\n",
    "    string_value.lower()\n",
    "    file_arr = string_value.split(\" \")\n",
    "    new_file_contents = []\n",
    "    for word in file_arr:\n",
    "        word = word.lower()\n",
    "        word = word.lstrip()\n",
    "        word = word.strip(\"\\n\")\n",
    "        word = word.replace(\",\",\"\")\n",
    "        word = word.replace(\" \",\"\")\n",
    "        word = word.replace(\"_\",\"\")\n",
    "        word = word.replace(\"?\",\"\")\n",
    "        word = word.replace(\"!\",\"\")\n",
    "        word = word.replace(\"#\",\"\")\n",
    "        word = word.replace(\"â€¦\",\"\")\n",
    "        word = re.sub('\\+', ' ',word)\n",
    "        word = re.sub('.*\\+\\n', '',word)\n",
    "        word = re.sub('zz+', ' ',word)\n",
    "        word=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', word, flags=re.IGNORECASE)\n",
    "        word=re.sub(' +', ' ', word, flags=re.IGNORECASE)\n",
    "        word=re.sub(r'\\\"', ' ', word, flags=re.IGNORECASE)\n",
    "        word=re.sub(r'[^a-zA-Z]', \" \", word, flags=re.VERBOSE)\n",
    "        word = word.replace(\"\\t\",\"\")\n",
    "        word = word.replace(\".\",\"\")\n",
    "        word = word.strip()\n",
    "        word = word.replace(\"'\\'\",\"\")\n",
    "        if word not in [\"\", \"\\\\\", '\"', \"'\", \"*\", \":\", \";\"]:\n",
    "            if not re.search(r'\\d', word):\n",
    "                if word not in stop_words:\n",
    "                    new_file_contents.append(word)\n",
    "        file_string_join = \" \".join(new_file_contents)\n",
    "        file_string_join = file_string_join.replace(\"\\\\n\",\"\")\n",
    "        file_string_join = file_string_join.strip(\"\\\\n\")\n",
    "        file_string_join = file_string_join.replace(\"\\\\'\",\"\")\n",
    "        file_string_join = file_string_join.replace(\"\\\\\",\"\")\n",
    "        file_string_join = file_string_join.replace('\"',\"\")\n",
    "        file_string_join = file_string_join.replace(\"'\",\"\")\n",
    "        file_string_join = file_string_join.replace(\"s'\",\"\")\n",
    "        file_string_join = file_string_join.lstrip()\n",
    "    return str(file_string_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path.joinpath(twitter_data)\n",
    "MyFILE=open(file_path,\"w\")\n",
    "WriteThis=\"Label,Tweet ID,Tweet Text\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "tweets =[]\n",
    "for topic in twitter_labels:\n",
    "    if topic in tweets_jsontxt:\n",
    "        fetched_tweets = tweets_jsontxt[topic]\n",
    "        # parsing tweets one by one\n",
    "        for tweet in fetched_tweets:\n",
    "            text = clean_content(str(tweet.text))\n",
    "            tweet_id = tweet.id    \n",
    "            # if tweet has retweets, ensure that it is appended only once\n",
    "            if text not in tweets:\n",
    "                tweets.append(text)\n",
    "                WriteThis=str(topic)+\",\"+str(tweet_id)+\",\"+str(text)+\"\\n\" \n",
    "                MyFILE.write(WriteThis)            \n",
    "MyFILE.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetched_tweets = api.search_tweets(q = \"poonam\",lang=\"en\", count = 100,tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afe57345d6b78de11303afd6d7a92989906a5fc5cba5d6e46dc784aa2e3ade50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
