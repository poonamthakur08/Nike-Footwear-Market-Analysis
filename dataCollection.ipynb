{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  ## for getting data from a server GET\n",
    "import re   ## for regular expressions   \n",
    "import numpy as np\n",
    "import pandas as pd ## for dataframes and related\n",
    "from pandas import DataFrame\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html  \n",
    "import csv\n",
    "from time import sleep\n",
    "import argparse\n",
    "import sys\n",
    "import time as t\n",
    "from pathlib import Path\n",
    "\n",
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "import nltk\n",
    "#import pycountry\n",
    "import string\n",
    "from PIL import Image\n",
    "\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "import configparser\n",
    "import tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project\\CORPUS')\n",
    "project_path = Path('E:\\Masters_Coursework\\Sem4\\Text Mining\\Project')\n",
    "labels = [\"+Nike +footwear +market +ranking\",\n",
    "\"+global +footwear +market +leader\",\n",
    "\"+top +brand in +footwear +industry\",\n",
    "\"+Nike +market +share in +footwear\",\n",
    "\"Nike footwear\",\n",
    "\"footwear industry\"]\n",
    "endpoint=\"https://newsapi.org/v2/everything\"\n",
    "filename=\"newsApiData.csv\"\n",
    "nytimes_data =  \"fav_sneaker_data.txt\"\n",
    "nike_vs_adidas_data =  \"nike_vs_adidas_data.txt\"\n",
    "nike_reviews_data = \"nike_reviews_data.csv\"\n",
    "twitter_data = \"twitter_data.csv\"\n",
    "twitter_labels = [\"footwear market\",\n",
    "\"global footwear leader\",\n",
    "\"top brand in footwear\",\n",
    "\"Nike market\",\n",
    "\"Nike footwear\",\n",
    "\"footwear industry\",\n",
    "\"Leading footwear brand\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEWS API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontxt = {}\n",
    "for topic in labels:\n",
    "    ## Dictionary Structure\n",
    "    URLPost = {'apiKey':'b1c5c9b6caec41e4933b5c6c76c4051b',\n",
    "                'sort_by':'relevancy',\n",
    "               'q':topic,\n",
    "               'pageSize':100\n",
    "    }\n",
    "\n",
    "    response=requests.get(endpoint, URLPost)\n",
    "    print(response)\n",
    "    jsontxt[topic] = response.json()\n",
    "    #print(jsontxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(text):\n",
    "    text=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(' +', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(r'\\\"', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(r'[^a-zA-Z]', \" \", text, flags=re.VERBOSE)\n",
    "    text=text.replace(',', '')\n",
    "    text=' '.join(text.split())\n",
    "    text=re.sub(\"\\n|\\r\", \"\", text)\n",
    "    text= ' '.join([wd for wd in text.split() if len(wd)>3])\n",
    "    return text\n",
    "\n",
    "file_path = project_path.joinpath(filename)\n",
    "MyFILE=open(file_path,\"w\")\n",
    "WriteThis=\"Label,Date,Author,Source,Title,Url,Headline,Content\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "for topic in labels:\n",
    "    articles = jsontxt[topic][\"articles\"]\n",
    "    label = re.sub('[+]',\"\",topic) if \"+\" in topic else topic\n",
    "    for i in range(len(articles)):              \n",
    "        Date=articles[i][\"publishedAt\"]\n",
    "        NewDate=Date.split(\"T\")\n",
    "        Date=NewDate[0]\n",
    "\n",
    "        Author=items[\"author\"]\n",
    "        Author=str(Author)\n",
    "        Author=Author.replace(',', '')\n",
    "\n",
    "        Source=articles[i][\"source\"][\"name\"]\n",
    "\n",
    "        Title=articles[i][\"title\"]\n",
    "        Title=clean_content(str(Title))\n",
    "        \n",
    "        url = articles[i][\"url\"]\n",
    "\n",
    "        Headline=articles[i][\"description\"]\n",
    "        Headline=clean_content(str(Headline))\n",
    "\n",
    "        Content=articles[i][\"content\"]\n",
    "        Content=clean_content(str(Content))\n",
    "        \n",
    "        WriteThis=str(label)+\",\"+str(Date)+\",\"+str(Author)+\",\"+str(Source)+\",\"+ str(Title) + \",\" + str(Url) + \",\" + str(Headline) +\",\" + str(Content) + \"\\n\" \n",
    "        MyFILE.write(WriteThis)\n",
    "            \n",
    "MyFILE.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Websites used for scrapping\n",
    "# https://www.nytimes.com/wirecutter/guides/our-favorite-sneakers/\n",
    "# https://www.trustpilot.com/review/www.nike.com \n",
    "# https://seekingalpha.com/article/4516865-nike-vs-adidas-an-undisputed-leader\n",
    "# https://www.feedough.com/10-biggest-nike-competitors/\n",
    "# https://travel.earth/adidas-vs-nike/ \n",
    "# https://www.unacast.com/post/nike-adidas-puma-foot-traffic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_soupdata(url_link):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36'}\n",
    "    response = requests.get(url_link,headers=headers).text\n",
    "    return BeautifulSoup(response,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.nytimes.com/wirecutter/guides/our-favorite-sneakers/'\n",
    "nytimes_soup = get_website_soupdata(url)\n",
    "info = nytimes_soup.find_all('div',attrs = {'class':'ca523f26 _7cb53a9f _614adc05'})\n",
    "fav_sneakers = []\n",
    "if info is not None and len(info)>0 :\n",
    "    for record in info:\n",
    "        para = record.find('p')\n",
    "        if para is not None:\n",
    "            fav_sneakers.append(para.text) \n",
    "\n",
    "    if len(fav_sneakers) > 0:\n",
    "        file_path = corpus_path.joinpath(nytimes_data)\n",
    "        MyFILE=open(file_path,\"w\")\n",
    "        WriteThis=\"Blog Content\\n\"\n",
    "        MyFILE.write(WriteThis)\n",
    "        MyFILE.writelines(fav_sneakers)\n",
    "        MyFILE.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://travel.earth/adidas-vs-nike/'\n",
    "adidas_nike_soup = get_website_soupdata(url)\n",
    "nike_adidas_info = adidas_nike_soup.find_all('div',attrs = {'class':'td-post-content tagdiv-type'})\n",
    "nike_adidas = []\n",
    "if nike_adidas_info is not None and len(nike_adidas_info)>0 :\n",
    "    for record in nike_adidas_info:\n",
    "        all_para = record.find_all('p')\n",
    "        for val in all_para:\n",
    "            if val is not None:\n",
    "                nike_adidas.append(val.text) \n",
    "\n",
    "    if len(nike_adidas) > 0:\n",
    "        file_path = corpus_path.joinpath(nike_vs_adidas_data)\n",
    "        MyFILE=open(file_path,\"w\",encoding=\"utf-8\")\n",
    "        WriteThis=\"Blog Content\\n\"\n",
    "        MyFILE.write(WriteThis)\n",
    "        MyFILE.writelines(nike_adidas)\n",
    "        MyFILE.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.trustpilot.com/review/www.nike.com'\n",
    "\n",
    "def get_all_reviews_url(url):\n",
    "    all_url_list = []\n",
    "    all_url_list.append(url)\n",
    "    for i in range(0,15):\n",
    "        all_url_list.append(url+'?page='+str(i+2))\n",
    "    return all_url_list\n",
    "\n",
    "all_review_urls = get_all_reviews_url(url)\n",
    "file_path = project_path.joinpath(nike_reviews_data)\n",
    "MyFILE=open(file_path,\"w\",encoding=\"utf-8\")\n",
    "WriteThis=\"Title,Rating,Review\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "for url in all_review_urls:\n",
    "    nike_soup = get_website_soupdata(url)\n",
    "    nike_reviews = nike_soup.find_all('section',attrs = {'class':'styles_reviewContentwrapper__zH_9M'})\n",
    "    if nike_reviews is not None and len(nike_reviews)>0 :\n",
    "        \n",
    "        for record in nike_reviews:\n",
    "            title_h2 = record.find('h2',attrs = {'class':'typography_heading-s__f7029 typography_appearance-default__AAY17'})\n",
    "            rating_img = record.find('img')\n",
    "            para_p = record.find('p',attrs = {'class':'typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'})\n",
    "            WriteThis= (str(title_h2.text) if title_h2 is not None else 'N/A') + \",\" + (str(rating_img['alt']) if rating_img is not None else 'N/A') + \",\" + (str(para_p.text) if para_p is not None else 'N/A')+\"\\n\"   \n",
    "            MyFILE.write(WriteThis)       \n",
    "        ## CLOSE THE FILE\n",
    "MyFILE.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWITTER API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key = config['twitter']['consumerKey']\n",
    "api_key_secret = config['twitter']['consumerSecret']\n",
    "\n",
    "access_token = config['twitter']['accessToken']\n",
    "access_token_secret = config['twitter']['accessTokenSecret']\n",
    "\n",
    "\n",
    "# authenticate\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_jsontxt = {}\n",
    "for topic in twitter_labels:\n",
    "    topic = re.sub('[+]',\"\",topic) if \"+\" in topic else topic\n",
    "    try:\n",
    "        fetched_tweets = api.search_tweets(q = topic,lang=\"en\", count = 100,text: \"full_text\")\n",
    "        tweets_jsontxt[topic] = fetched_tweets\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(text):\n",
    "    text=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(' +', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(r'\\\"', ' ', text, flags=re.IGNORECASE)\n",
    "    text=re.sub(r'[^a-zA-Z]', \" \", text, flags=re.VERBOSE)\n",
    "    text=text.replace(',', '')\n",
    "    text=' '.join(text.split())\n",
    "    text=re.sub(\"\\n|\\r\", \"\", text)\n",
    "    text= ' '.join([wd for wd in text.split() if len(wd)>3])\n",
    "    return text\n",
    "\n",
    "file_path = project_path.joinpath(twitter_data)\n",
    "MyFILE=open(file_path,\"w\")\n",
    "WriteThis=\"Label,Tweet ID,Tweet Text\\n\"\n",
    "MyFILE.write(WriteThis)\n",
    "tweets =[]\n",
    "for topic in twitter_labels:\n",
    "    if topic in tweets_jsontxt:\n",
    "        fetched_tweets = tweets_jsontxt[topic]\n",
    "        # parsing tweets one by one\n",
    "        for tweet in fetched_tweets:\n",
    "            text = clean_content(str(tweet.text))\n",
    "            tweet_id = tweet.id    \n",
    "            # if tweet has retweets, ensure that it is appended only once\n",
    "            if text not in tweets:\n",
    "                tweets.append(text)\n",
    "                WriteThis=str(topic)+\",\"+str(tweet_id)+\",\"+str(text)+\"\\n\" \n",
    "                MyFILE.write(WriteThis)            \n",
    "MyFILE.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetched_tweets = api.search_tweets(q = \"poonam\",lang=\"en\", count = 100,tweet_mode='extended')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc7a5a3ce4ebf1c60b6a449fa3a9318bbc0e30cf0c24cfb5c56f66eefe8fea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
